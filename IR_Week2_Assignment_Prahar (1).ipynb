{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. NLTK Initialization and Setup:"
      ],
      "metadata": {
        "id": "wFX5u6d-m7Hq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qSGi_SmEa68_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "76983b99-2e49-4405-f176-932e28fb7943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import os\n",
        "import string\n",
        "import logging\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "LEMMATIZER = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Loading Text Files:"
      ],
      "metadata": {
        "id": "nlxKBdETnTBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_files(folder_path):\n",
        "    \"\"\" Reads all .txt files in a folder and returns a dictionary with filenames as keys and content as values. \"\"\"\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0  # Initialize document ID\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data[doc_id] = file.read()\n",
        "            doc_id_to_filename[doc_id] = filename  # Map doc_id to filename\n",
        "            logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "            doc_id += 1  # Increment document ID for the next file\n",
        "    return data, doc_id_to_filename\n",
        "\n",
        "# Example folder path: Replace this with your folder path\n",
        "folder_path = \"/content/drive/MyDrive/Information Retrieval System/Week2/docs\"\n",
        "data, doc_id_to_filename = load_text_files(folder_path)\n"
      ],
      "metadata": {
        "id": "KLVlts_MnTeZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Text Cleaning Pipeline:"
      ],
      "metadata": {
        "id": "B5Nkr_WkneSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Performs text cleaning: removing special characters, tokenization, stop word removal, and lemmatization.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    # Tokenize the cleaned text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Test cleaning function with a sample text\n",
        "print(clean_text(data[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h1bz5iBrnepb",
        "outputId": "481911da-a0e6-47e1-bd23-40bfef9ee4b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['applesupport', 'many', 'user', 'reporting', 'issue', 'latest', 'io', 'update', 'battery', 'life', 'seems', 'drain', 'faster', 'apps', 'like', 'safari', 'crashing', 'frequently', 'user', 'tried', 'reinstalling', 'update', 'doesnt', 'seem', 'resolve', 'problem', 'check', 'discussion', 'httpssupportapplecomiosupdates', 'iosupdate', 'appleproblems', '105836', 'working', 'ok', 'miriam', 'link', 'help', 'httpstco0m2mph15eh', 'mm', 'virgintrains', 'still', 'havent', 'heard', 'amp', 'number', 'im', 'directed', 'phone', 'dead', 'end', 'amp', 'live', 'chat', 'doesnt', 'work', 'someone', 'call', '105836', 'thats', 'miriam', 'team', 'send', 'email', 'shortly', 'hp', '105837', 'help', 'version', 'io', 'find', 'setting', 'gt', 'general', 'gt', 'reply', 'dm', 'httpstcogdrqu22ypt', '105838', 'applesupport', 'suffering', 'hope', 'find', 'solution', 'applesupport', 'hi', 'apple', 'ive', 'concern', 'latest', 'io', 'slow', 'iphone6', 'happy', 'solution', 'please', '105839', 'thanks', 'reaching', 'u', 'always', 'happy', 'help', 'send', 'u', 'dm', 'look', 'together', 'httpstcogdrqu22ypt', '76099', 'plz', 'beg', 'sort', 'battery', 'life']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Inverted Index Construction"
      ],
      "metadata": {
        "id": "zzmiL4FmqBV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(data):\n",
        "    \"\"\"Builds an inverted index from the cleaned text data and tracks term frequencies.\"\"\"\n",
        "    inverted_index = defaultdict(set)\n",
        "    term_frequencies = Counter()  # Track the frequency of each term\n",
        "    for doc_id, content in data.items():\n",
        "        cleaned_tokens = clean_text(content)\n",
        "        for token in cleaned_tokens:\n",
        "            inverted_index[token].add(doc_id)\n",
        "            term_frequencies[token] += 1  # Update term frequency\n",
        "    return inverted_index, term_frequencies\n",
        "\n",
        "inverted_index, term_frequencies = build_inverted_index(data)\n"
      ],
      "metadata": {
        "id": "wnX7fJgZp7Iu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Boolean Query Processing: AND, OR, and NOT Operations:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_67vXOdtqPvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_query(query, inverted_index, doc_id_to_filename):\n",
        "    \"\"\"Processes a Boolean query and returns the matching documents.\"\"\"\n",
        "    tokens = query.lower().split()\n",
        "    result_set = set()\n",
        "\n",
        "    if 'and' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        result_set = inverted_index[terms[0]]\n",
        "        for term in terms[1:]:\n",
        "            if term in inverted_index:\n",
        "                result_set = result_set.intersection(inverted_index[term])\n",
        "\n",
        "    elif 'or' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        for term in terms:\n",
        "            if term in inverted_index:\n",
        "                if not result_set:\n",
        "                    result_set = inverted_index[term]\n",
        "                else:\n",
        "                    result_set = result_set.union(inverted_index[term])\n",
        "\n",
        "    elif 'not' in tokens:\n",
        "        term = tokens[1]\n",
        "        if term in inverted_index:\n",
        "            result_set = set(inverted_index.keys()) - inverted_index[term]\n",
        "        else:\n",
        "            result_set = set(inverted_index.keys())\n",
        "\n",
        "    else:\n",
        "        if query in inverted_index:\n",
        "            result_set = inverted_index[query]\n",
        "        else:\n",
        "            result_set = set()\n",
        "\n",
        "    # Convert doc_ids to filenames\n",
        "    result_filenames = [doc_id_to_filename[doc_id] for doc_id in result_set if doc_id in doc_id_to_filename]\n",
        "    logging.info(f\"Query '{query}' resulted in: {result_filenames}\")\n",
        "    return result_filenames\n"
      ],
      "metadata": {
        "id": "FwMfJy4sqQzN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Main Function:"
      ],
      "metadata": {
        "id": "ZtCpaTtWqZsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_results_to_file(queries, inverted_index, doc_id_to_filename, output_file_path):\n",
        "    \"\"\"Writes the query results to a text file at the specified location.\"\"\"\n",
        "    with open(output_file_path, \"w\") as result_file:\n",
        "        for query in queries:\n",
        "            result = boolean_query(query, inverted_index, doc_id_to_filename)\n",
        "            result_str = f\"Results for '{query}': {result}\\n\"\n",
        "            print(result_str)  # Print to console\n",
        "            result_file.write(result_str)  # Write to file\n"
      ],
      "metadata": {
        "id": "aELR64jFw5GE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load dataset\n",
        "    folder_path = \"/content/drive/MyDrive/Information Retrieval System/Week2/docs\"\n",
        "    data, doc_id_to_filename = load_text_files(folder_path)\n",
        "\n",
        "    # Build the inverted index and term frequencies\n",
        "    inverted_index, term_frequencies = build_inverted_index(data)\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"Netflix AND Xbox\",\n",
        "        \"Microsoft OR Windows\",\n",
        "        \"NOT Xbox\",\n",
        "        \"Xbox\",\n",
        "        \"Windows AND NOT Netflix\",\n",
        "        \"Spotify AND Bluetooth\",\n",
        "        \"Apple OR Battery\",\n",
        "        \"Virgin AND Train\",\n",
        "        \"Telemarketing AND O2\",\n",
        "        \"Spotify AND Bluetooth AND NOT Car\",\n",
        "        \"Battery AND Drain\",\n",
        "        \"Netflix OR Xbox\",\n",
        "        \"Spotify AND NOT Bluetooth\",\n",
        "        \"Windows OR Xbox\",\n",
        "        \"British AND Flight\",\n",
        "        \"Delayed AND Train\",\n",
        "        \"NOT Spotify\",\n",
        "        \"Flight AND Delay AND NOT Train\",\n",
        "        \"Safari AND Crash\",\n",
        "        \"iPhone AND NOT Telemarketing\",\n",
        "        \"Windows AND WiFi AND Freeze\",\n",
        "        \"Spotify AND Pause\"\n",
        "    ]\n",
        "\n",
        "     # Specifying the output folder path for saving query results\n",
        "    output_folder_path = \"/content/drive/MyDrive/Information Retrieval System/Week2/docs\"\n",
        "\n",
        "     # Create the output file path\n",
        "    output_file_path = os.path.join(output_folder_path, \"query_results.txt\")\n",
        "\n",
        "    # Write query results to the specified output file path\n",
        "    write_results_to_file(queries, inverted_index, doc_id_to_filename, output_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlm0DYFSqeaV",
        "outputId": "b54a1034-3727-4a2a-bc78-21cc5fe924c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for 'Netflix AND Xbox': ['query_results.txt']\n",
            "\n",
            "Results for 'Microsoft OR Windows': ['Docs5.txt', 'query_results.txt']\n",
            "\n",
            "Results for 'NOT Xbox': []\n",
            "\n",
            "Results for 'Xbox': []\n",
            "\n",
            "Results for 'Windows AND NOT Netflix': []\n",
            "\n",
            "Results for 'Spotify AND Bluetooth': ['query_results.txt']\n",
            "\n",
            "Results for 'Apple OR Battery': ['Docs1.txt', 'query_results.txt']\n",
            "\n",
            "Results for 'Virgin AND Train': ['query_results.txt']\n",
            "\n",
            "Results for 'Telemarketing AND O2': ['query_results.txt']\n",
            "\n",
            "Results for 'Spotify AND Bluetooth AND NOT Car': ['query_results.txt']\n",
            "\n",
            "Results for 'Battery AND Drain': ['Docs1.txt', 'query_results.txt']\n",
            "\n",
            "Results for 'Netflix OR Xbox': ['Docs3.txt', 'Docs7.txt', 'query_results.txt']\n",
            "\n",
            "Results for 'Spotify AND NOT Bluetooth': ['query_results.txt']\n",
            "\n",
            "Results for 'Windows OR Xbox': ['Docs7.txt', 'query_results.txt']\n",
            "\n",
            "Results for 'British AND Flight': ['query_results.txt']\n",
            "\n",
            "Results for 'Delayed AND Train': ['query_results.txt']\n",
            "\n",
            "Results for 'NOT Spotify': []\n",
            "\n",
            "Results for 'Flight AND Delay AND NOT Train': ['query_results.txt']\n",
            "\n",
            "Results for 'Safari AND Crash': ['query_results.txt']\n",
            "\n",
            "Results for 'iPhone AND NOT Telemarketing': ['query_results.txt']\n",
            "\n",
            "Results for 'Windows AND WiFi AND Freeze': []\n",
            "\n",
            "Results for 'Spotify AND Pause': ['query_results.txt']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}