{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. NLTK Initialization and Setup:"
      ],
      "metadata": {
        "id": "wFX5u6d-m7Hq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qSGi_SmEa68_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97503696-d9a0-4331-a907-f82a4825fb6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import os\n",
        "import string\n",
        "import logging\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "LEMMATIZER = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Loading Text Files:"
      ],
      "metadata": {
        "id": "nlxKBdETnTBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_files(folder_path):\n",
        "    \"\"\" Reads all .txt files in a folder and returns a dictionary with filenames as keys and content as values. \"\"\"\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0  # Initialize document ID\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data[doc_id] = file.read()\n",
        "            doc_id_to_filename[doc_id] = filename  # Map doc_id to filename\n",
        "            logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "            doc_id += 1  # Increment document ID for the next file\n",
        "    return data, doc_id_to_filename\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Information Retrieval System/Week2/docs\"\n",
        "data, doc_id_to_filename = load_text_files(folder_path)\n"
      ],
      "metadata": {
        "id": "KLVlts_MnTeZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Text Cleaning Pipeline:"
      ],
      "metadata": {
        "id": "B5Nkr_WkneSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning pipeline\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleaning the input text by removing special characters, tokenization, stop word removal, and lemmatization.\"\"\"\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS]  # Lemmatize and remove stopwords\n",
        "    return cleaned_tokens\n",
        "# Test cleaning function with a sample text\n",
        "print(clean_text(data[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1bz5iBrnepb",
        "outputId": "2f276fe4-d55a-4132-e05f-17458ecee113"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['applesupport', 'many', 'user', 'reporting', 'issue', 'latest', 'io', 'update', 'battery', 'life', 'seems', 'drain', 'faster', 'apps', 'like', 'safari', 'crashing', 'frequently', 'user', 'tried', 'reinstalling', 'update', 'doesnt', 'seem', 'resolve', 'problem', 'check', 'discussion', 'httpssupportapplecomiosupdates', 'iosupdate', 'appleproblems', '105836', 'working', 'ok', 'miriam', 'link', 'help', 'httpstco0m2mph15eh', 'mm', 'virgintrains', 'still', 'havent', 'heard', 'amp', 'number', 'im', 'directed', 'phone', 'dead', 'end', 'amp', 'live', 'chat', 'doesnt', 'work', 'someone', 'call', '105836', 'thats', 'miriam', 'team', 'send', 'email', 'shortly', 'hp', '105837', 'help', 'version', 'io', 'find', 'setting', 'gt', 'general', 'gt', 'reply', 'dm', 'httpstcogdrqu22ypt', '105838', 'applesupport', 'suffering', 'hope', 'find', 'solution', 'applesupport', 'hi', 'apple', 'ive', 'concern', 'latest', 'io', 'slow', 'iphone6', 'happy', 'solution', 'please', '105839', 'thanks', 'reaching', 'u', 'always', 'happy', 'help', 'send', 'u', 'dm', 'look', 'together', 'httpstcogdrqu22ypt', '76099', 'plz', 'beg', 'sort', 'battery', 'life']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Inverted Index Construction"
      ],
      "metadata": {
        "id": "zzmiL4FmqBV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(data):\n",
        "    \"\"\"Builds an inverted index from the cleaned text data and tracks term frequencies.\"\"\"\n",
        "    inverted_index = defaultdict(set)\n",
        "    term_frequencies = Counter()  # Track the frequency of each term\n",
        "    for doc_id, content in data.items():\n",
        "        cleaned_tokens = clean_text(content)\n",
        "        for token in cleaned_tokens:\n",
        "            inverted_index[token].add(doc_id)\n",
        "            term_frequencies[token] += 1  # Update term frequency\n",
        "    return inverted_index, term_frequencies\n",
        "\n",
        "inverted_index, term_frequencies = build_inverted_index(data)\n"
      ],
      "metadata": {
        "id": "wnX7fJgZp7Iu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Boolean Query Processing: AND, OR, and NOT Operations:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_67vXOdtqPvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_query(query, inverted_index, doc_id_to_filename):\n",
        "    \"\"\"Processes a boolean query using the inverted index and returns the matching document filenames.\"\"\"\n",
        "    query = query.lower()  # Convert query to lowercase\n",
        "    tokens = query.split()\n",
        "    result_set = set()\n",
        "\n",
        "    if 'and' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        for term in terms:\n",
        "            if term in inverted_index:\n",
        "                if not result_set:\n",
        "                    result_set = inverted_index[term]\n",
        "                else:\n",
        "                    result_set = result_set.intersection(inverted_index[term])\n",
        "            else:\n",
        "                result_set = set()  # If one term is not found, return an empty set\n",
        "\n",
        "    elif 'or' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        for term in terms:\n",
        "            if term in inverted_index:\n",
        "                if not result_set:\n",
        "                    result_set = inverted_index[term]\n",
        "                else:\n",
        "                    result_set = result_set.union(inverted_index[term])\n",
        "\n",
        "    elif 'not' in tokens:\n",
        "        term = tokens[1]\n",
        "        if term in inverted_index:\n",
        "            all_docs = set(doc_id_to_filename.keys())  # All document IDs\n",
        "            result_set = all_docs - inverted_index[term]\n",
        "        else:\n",
        "            result_set = set(doc_id_to_filename.keys())  # If term doesn’t exist, return all docs\n",
        "\n",
        "    else:\n",
        "        if query in inverted_index:\n",
        "            result_set = inverted_index[query]\n",
        "        else:\n",
        "            result_set = set()  # If query term doesn’t exist, return empty set\n",
        "\n",
        "    # Convert doc_ids to filenames\n",
        "    result_filenames = [doc_id_to_filename[doc_id] for doc_id in result_set if doc_id in doc_id_to_filename]\n",
        "    logging.info(f\"Query '{query}' resulted in: {result_filenames}\")\n",
        "    return result_filenames\n"
      ],
      "metadata": {
        "id": "FwMfJy4sqQzN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Main Function:"
      ],
      "metadata": {
        "id": "ZtCpaTtWqZsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_results_to_file(queries, inverted_index, doc_id_to_filename, output_file_path):\n",
        "    \"\"\"Writes the query results to a text file at the specified location.\"\"\"\n",
        "    with open(output_file_path, \"w\") as result_file:\n",
        "        for query in queries:\n",
        "            result = boolean_query(query, inverted_index, doc_id_to_filename)\n",
        "            result_str = f\"Results for '{query}': {result}\\n\"\n",
        "            print(result_str)  # Print to console\n",
        "            result_file.write(result_str)  # Write to file\n"
      ],
      "metadata": {
        "id": "aELR64jFw5GE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load dataset\n",
        "    folder_path = \"/content/drive/MyDrive/Information Retrieval System/Week2/docs\"\n",
        "    data, doc_id_to_filename = load_text_files(folder_path)\n",
        "\n",
        "    # Build the inverted index and term frequencies\n",
        "    inverted_index, term_frequencies = build_inverted_index(data)\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"netflix\",\n",
        "        \"microsoft OR windows\",\n",
        "        \"xbox\",\n",
        "        \"safari AND iosupdate\",\n",
        "        \"windows AND NOT netflix\",\n",
        "        \"spotify AND bluetooth\",\n",
        "        \"apple OR battery\",\n",
        "        \"battery AND drain\",\n",
        "        \"netflix OR xbox\",\n",
        "        \"NOT spotify\",\n",
        "        \"windows OR xbox\",\n",
        "\n",
        "    ]\n",
        "\n",
        "     # Specifying the output folder path for saving query results\n",
        "    output_folder_path = \"/content/drive/MyDrive/Information Retrieval System/Week2/docs\"\n",
        "\n",
        "     # Create the output file path\n",
        "    output_file_path = os.path.join(output_folder_path, \"Prahar_query_results_final.txt\")\n",
        "\n",
        "    # Write query results to the specified output file path\n",
        "    write_results_to_file(queries, inverted_index, doc_id_to_filename, output_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlm0DYFSqeaV",
        "outputId": "6a1dc4a7-c3d9-49c6-e142-704cbf0f1920"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for 'netflix': ['Docs3.txt']\n",
            "\n",
            "Results for 'microsoft OR windows': ['Docs5.txt']\n",
            "\n",
            "Results for 'xbox': ['Docs7.txt']\n",
            "\n",
            "Results for 'safari AND iosupdate': ['Docs1.txt']\n",
            "\n",
            "Results for 'windows AND NOT netflix': ['Docs3.txt']\n",
            "\n",
            "Results for 'spotify AND bluetooth': []\n",
            "\n",
            "Results for 'apple OR battery': ['Docs1.txt']\n",
            "\n",
            "Results for 'battery AND drain': ['Docs1.txt']\n",
            "\n",
            "Results for 'netflix OR xbox': ['Docs3.txt', 'Docs7.txt']\n",
            "\n",
            "Results for 'NOT spotify': ['Docs1.txt', 'Docs2.txt', 'Docs3.txt', 'Docs4.txt', 'Docs5.txt', 'Docs6.txt', 'Docs7.txt']\n",
            "\n",
            "Results for 'windows OR xbox': ['Docs7.txt']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}